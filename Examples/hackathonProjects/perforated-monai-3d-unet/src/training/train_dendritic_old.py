import os
os.environ["PYTHONBREAKPOINT"] = "0"

import sys
import torch
import wandb
import builtins

# hard-disable breakpoint everywhere
builtins.breakpoint = lambda *args, **kwargs: None

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.insert(0, ROOT_DIR)

import bootstrap

from perforatedai import utils_perforatedai as UPA
from perforatedai import globals_perforatedai as GPA
from monai.losses import DiceCELoss
from monai.metrics import DiceMetric
from monai.transforms import AsDiscrete
from monai.inferers import sliding_window_inference
from monai.networks.nets import UNet

from src.data.dataset_loader import get_dataloaders

# ========================
# CONFIG
# ========================
PROJECT_NAME = "Perforated-MONAI"
RUN_NAME = "dendritic_unet"

DATA_DIR = "datasets/monai"
NUM_EPOCHS = 30
#MAX_TOTAL_EPOCHS = 50
LR = 5e-5
PATCH_SIZE = (96, 96, 96)
NUM_CLASSES = 4
DEVICE = "cuda"


def flatten_if_needed(x):
    # [B,S,C,D,H,W] â†’ [B*S,C,D,H,W]
    if x.ndim == 6:
        return x.flatten(0, 1)
    return x


def main():
    os.makedirs("checkpoints/dendritic", exist_ok=True)
    wandb.init(project=PROJECT_NAME, name=RUN_NAME)

    # ============================================================
    # ðŸ”´ PAI CONFIG â€” MUST BE SET *BEFORE* initialize_pai
    # ============================================================
    GPA.pc.set_testing_dendrite_capacity(False)
    
    GPA.pc.set_max_dendrites(2)

    GPA.pc.set_perforated_backpropagation(False)
    GPA.pc.set_module_names_to_convert(["Conv3d"])
    GPA.pc.set_unwrapped_modules_confirmed(True)
    GPA.pc.set_weight_decay_accepted(True)
    GPA.pc.set_verbose(False)
    GPA.pc.set_improvement_threshold(0.01)

    # ========================
    # MODEL
    # ========================
    model = UNet(
        spatial_dims=3,
        in_channels=4,
        out_channels=NUM_CLASSES,
        channels=(32, 64, 128, 256),
        strides=(2, 2, 2),
        num_res_units=2,
    ).to(DEVICE)

    # ðŸ”´ tracker is created HERE
    model = UPA.initialize_pai(
        model,
        save_name="PAI_MONAI",
        maximizing_score=True,
    )
    #model = torch.compile(model, mode="max-autotune")
    GPA.pc.set_switch_mode("DOING_HISTORY")   
    # Required for Conv3D dendrites
    for m in model.modules():
        if hasattr(m, "set_this_output_dimensions"):
            m.set_this_output_dimensions([-1, 0, -1, -1, -1])

    # ========================
    # OPTIMIZER + SCHEDULER
    # ========================
    GPA.pai_tracker.set_optimizer(torch.optim.Adam)
    optimizer = GPA.pai_tracker.setup_optimizer(
        model,
        {"params": model.parameters(), "lr": LR},
    )
    # ========================
    # DATA
    # ========================
    train_loader, val_loader = get_dataloaders(DATA_DIR)
    steps_per_epoch = len(train_loader)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=NUM_EPOCHS * steps_per_epoch
    )


    # ========================
    # LOSS & METRICS
    # ========================
    loss_fn = DiceCELoss(
        to_onehot_y=True,
        softmax=True,
        include_background=False,
        lambda_dice=1.0,
        lambda_ce=0.5,
    )

    dice_metric = DiceMetric(
        include_background=False,
        reduction="mean_batch",
        get_not_nans=False,
    )

    post_pred = AsDiscrete(argmax=True, to_onehot=NUM_CLASSES)



    # ========================
    # TRAIN LOOP
    # ========================
    epoch = 0
    while True:
        model.train()
        epoch_loss = 0.0

        for batch in train_loader:
            inputs = flatten_if_needed(batch["image"]).to(DEVICE, non_blocking=True)
            labels = flatten_if_needed(batch["label"]).to(DEVICE, non_blocking=True)


            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast("cuda"):
                outputs = model(inputs)
                loss = loss_fn(outputs, labels)

            loss.backward()
            optimizer.step()
            scheduler.step()

            epoch_loss += loss.item()

        epoch_loss /= len(train_loader)

        # ========================
        # VALIDATION
        # ========================
        model.eval()
        dice_metric.reset()

        with torch.no_grad():
            for batch in val_loader:
                inputs = flatten_if_needed(batch["image"].cuda())
                labels = flatten_if_needed(batch["label"].cuda())

                if labels.ndim == 5:
                    labels = labels.squeeze(1)
                labels = labels.long()

                with torch.amp.autocast("cuda"):
                    outputs = sliding_window_inference(
                        inputs,
                        PATCH_SIZE,
                        sw_batch_size=2,
                        predictor=model,
                    )

                outputs = post_pred(outputs)
                dice_metric(y_pred=outputs, y=labels)

        dice_vals = dice_metric.aggregate()
        mean_dice = dice_vals.mean().item()

        # ========================
        # DENDRITE TRIGGER
        # ========================
        model, restructured, training_complete = GPA.pai_tracker.add_validation_score(
            mean_dice, model
        )
        model = model.to(DEVICE)

        if restructured and not training_complete:
            GPA.pai_tracker.set_optimizer(torch.optim.Adam)
            optimizer = GPA.pai_tracker.setup_optimizer(
                model, {"params": model.parameters(), "lr": LR}
            )
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=NUM_EPOCHS * steps_per_epoch
            )

            print("ðŸ§  Dendrites grown â†’ optimizer & scheduler reset")

        wandb.log({
            "epoch": epoch,
            "train/loss": epoch_loss,
            "val/dice": mean_dice,
            "dice/WT": dice_vals[0].item(),
            "dice/TC": dice_vals[1].item(),
            "dice/ET": dice_vals[2].item(),
            "dendrites": GPA.pai_tracker.member_vars.get("num_dendrites_added", 0),
            "lr": scheduler.get_last_lr()[0],
        })

        print(
            f"Epoch {epoch+1} | "
            f"Loss {epoch_loss:.4f} | "
            f"Dice {mean_dice:.4f}"
        )

        epoch += 1

        # ðŸ”´ HARD STOP (independent of PAI)
#        if epoch >= MAX_TOTAL_EPOCHS:
#            print(f"ðŸ›‘ Hard stop reached at epoch {epoch}")
#            break


        if training_complete:
            print("âœ… Training complete according to PAI tracker")
            break
    
    # ========================
    # SAVE BEST MODEL
    # ========================
    # PAI already restored best weights internally
    model = model.to(DEVICE)

    torch.save(
        model.state_dict(),
        "checkpoints/dendritic/unet_dendritic_new.pt",
    )

    wandb.finish()


if __name__ == "__main__":
    main()
